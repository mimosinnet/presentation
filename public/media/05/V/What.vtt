WEBVTT
Kind: captions
Language: en

00:00:03.730 --> 00:00:06.140
Imagine 2 machines.

00:00:06.140 --> 00:00:12.960
They both output messages from
an alphabet of A, B, C, or D.

00:00:12.960 --> 00:00:15.590
Machine 1 generates
each symbol randomly.

00:00:15.590 --> 00:00:19.000
They all occur 25% of the time.

00:00:19.000 --> 00:00:21.082
Machine 2 generates
symbols according

00:00:21.082 --> 00:00:22.373
to the following probabilities.

00:00:25.540 --> 00:00:30.370
Now, which machine is
producing more information?

00:00:30.370 --> 00:00:34.430
Claude Shannon cleverly
rephrased the question.

00:00:34.430 --> 00:00:39.200
If you had to predict the
next symbol from each machine,

00:00:39.200 --> 00:00:42.330
what is the minimum number
of yes or no questions

00:00:42.330 --> 00:00:45.340
you would expect to ask?

00:00:45.340 --> 00:00:47.200
Let's look at Machine 1.

00:00:47.200 --> 00:00:50.020
The most efficient way is
to pose a question which

00:00:50.020 --> 00:00:52.860
divides the
possibilities in half.

00:00:52.860 --> 00:00:55.280
For example, our
first question, we

00:00:55.280 --> 00:00:58.670
could ask if it is any
2 symbols-- such as,

00:00:58.670 --> 00:01:00.420
is it A or B?

00:01:00.420 --> 00:01:03.350
Since there is a
50% chance of A or B

00:01:03.350 --> 00:01:08.020
and a 50% chance of C or D.
After getting the answer,

00:01:08.020 --> 00:01:10.760
we can eliminate half
of the possibilities.

00:01:10.760 --> 00:01:15.260
And we will be left with 2
symbols, both equally likely.

00:01:15.260 --> 00:01:18.920
So we simply pick
one-- such as, is it A?

00:01:18.920 --> 00:01:20.990
And after this
second question, we

00:01:20.990 --> 00:01:23.570
will have correctly
identified the symbol.

00:01:23.570 --> 00:01:26.720
So we can say the
uncertainty of Machine 1

00:01:26.720 --> 00:01:30.560
is 2 questions per symbol.

00:01:30.560 --> 00:01:32.370
Now, what about Machine 2?

00:01:32.370 --> 00:01:36.170
As with Machine 1, we
could ask two questions

00:01:36.170 --> 00:01:38.410
to determine the next symbol.

00:01:38.410 --> 00:01:41.020
However, this time the
probability of each symbol

00:01:41.020 --> 00:01:41.980
is different.

00:01:41.980 --> 00:01:45.060
So we can ask our
questions differently.

00:01:45.060 --> 00:01:48.500
Here A has a 50%
chance of occurring,

00:01:48.500 --> 00:01:51.690
and all other
letters add to 50%.

00:01:51.690 --> 00:01:55.070
So we could start
by asking-- Is it A?

00:01:55.070 --> 00:01:57.330
If it is A, we are done.

00:01:57.330 --> 00:01:59.860
Only one question in this case.

00:01:59.860 --> 00:02:06.180
Otherwise, we are left with 2
equal outcomes-- D or B and C.

00:02:06.180 --> 00:02:09.410
So we could ask-- is it D?

00:02:09.410 --> 00:02:12.730
If yes, we are done
with 2 questions.

00:02:12.730 --> 00:02:15.440
Otherwise, we have to
ask a third question

00:02:15.440 --> 00:02:18.670
to identify which of the
last 2 symbols it is.

00:02:23.390 --> 00:02:26.450
On average, how many
questions do you

00:02:26.450 --> 00:02:31.690
expect to ask to determine
a symbol from Machine 2?

00:02:31.690 --> 00:02:34.940
And this can be explained
nicely with an analogy.

00:02:34.940 --> 00:02:40.550
Let's assume instead we want to
build Machine 1 and Machine 2.

00:02:40.550 --> 00:02:44.980
And we can generate symbols
by bouncing a disk off a peg

00:02:44.980 --> 00:02:48.140
into 1 of 2 equally
likely directions.

00:02:48.140 --> 00:02:51.780
Based on which way it falls,
we can generate a symbol.

00:02:51.780 --> 00:02:54.060
So with Machine
1, we need to add

00:02:54.060 --> 00:02:56.750
a second level or
a second bounce

00:02:56.750 --> 00:03:00.000
so that we have 2
bounces, which lead

00:03:00.000 --> 00:03:02.440
to 4 equally likely outcomes.

00:03:02.440 --> 00:03:07.060
And based on where the disk
lands, we output A, B, C, or D.

00:03:07.060 --> 00:03:08.650
Now, Machine 2.

00:03:08.650 --> 00:03:10.540
In this case, the
first bounce leads

00:03:10.540 --> 00:03:15.050
to either an A-- which occurs
50% of the time-- or else

00:03:15.050 --> 00:03:18.250
we lead to a second bounce,
which then can either output

00:03:18.250 --> 00:03:22.020
at D-- which occurs 25%
of the time-- or else

00:03:22.020 --> 00:03:25.735
it leads to a third bounce,
which then leads to either B

00:03:25.735 --> 00:03:29.880
or C-- 12.5% of the time.

00:03:29.880 --> 00:03:31.610
So now we just take
a weighted average

00:03:31.610 --> 00:03:35.130
as follows-- the expected
number of bounces

00:03:35.130 --> 00:03:37.590
is the probability
of symbol A times 1

00:03:37.590 --> 00:03:41.710
bounce plus the probability
of B times 3 bounces

00:03:41.710 --> 00:03:44.540
plus the probability
of C times 3 bounces

00:03:44.540 --> 00:03:47.880
plus the probability
of D times 2 bounces.

00:03:47.880 --> 00:03:52.480
And this works out
to 1.75 bounces.

00:03:52.480 --> 00:03:54.830
Now, notice the
connection between yes

00:03:54.830 --> 00:03:57.250
or no questions
and fair bounces.

00:03:57.250 --> 00:04:00.120
The expected number
of questions is

00:04:00.120 --> 00:04:03.920
equal to the expected
number of bounces.

00:04:03.920 --> 00:04:07.780
So Machine 1 requires 2
bounces to generate a symbol

00:04:07.780 --> 00:04:11.700
while guessing an unknown
symbol requires 2 questions.

00:04:11.700 --> 00:04:15.040
Machine 2 requires 1.75 bounces.

00:04:15.040 --> 00:04:20.410
We need to ask 1.75
questions on average.

00:04:20.410 --> 00:04:24.360
Meaning, if we need to guess
100 symbols from both machines,

00:04:24.360 --> 00:04:28.920
we can expect to ask 200
questions for Machine 1

00:04:28.920 --> 00:04:33.160
and 175 questions for Machine 2.

00:04:33.160 --> 00:04:38.050
So this means that Machine 2
is producing less information

00:04:38.050 --> 00:04:40.790
because there is
less uncertainty

00:04:40.790 --> 00:04:43.370
or surprise about its output.

00:04:43.370 --> 00:04:44.570
And that's it.

00:04:44.570 --> 00:04:48.860
Claude Shannon calls this
measure of average uncertainty

00:04:48.860 --> 00:04:54.170
"entropy," and he uses a
letter H to represent it.

00:04:54.170 --> 00:04:57.420
And the unit of
entropy Shannon chooses

00:04:57.420 --> 00:05:01.030
is based on the uncertainty
of a fair coin flip,

00:05:01.030 --> 00:05:03.750
and he calls this
"the bit," which

00:05:03.750 --> 00:05:05.350
is equivalent to a fair bounce.

00:05:08.640 --> 00:05:11.200
And we can arrive at the
same result using our bounce

00:05:11.200 --> 00:05:12.230
analogy.

00:05:12.230 --> 00:05:15.640
Entropy, or H, is the
summation for each symbol

00:05:15.640 --> 00:05:17.290
of the probability
of that symbol

00:05:17.290 --> 00:05:19.670
times the number of bounces.

00:05:19.670 --> 00:05:21.450
Now, the difference
is-- how do we

00:05:21.450 --> 00:05:25.020
express number of bounces
in a more general way?

00:05:25.020 --> 00:05:27.120
And as we've seen,
number of bounces

00:05:27.120 --> 00:05:29.880
depends how far down
the tree we are.

00:05:29.880 --> 00:05:31.670
And we can simplify
this by saying

00:05:31.670 --> 00:05:37.010
that the number of bounces
equals the logarithm base 2

00:05:37.010 --> 00:05:39.180
of the number of
outcomes at that level.

00:05:39.180 --> 00:05:41.770
And the number of
outcomes at a level

00:05:41.770 --> 00:05:44.820
is also based on the
probability, where

00:05:44.820 --> 00:05:46.830
the number of
outcomes at a level

00:05:46.830 --> 00:05:50.810
equals 1 divided by the
probability of that outcome.

00:05:50.810 --> 00:05:56.200
Number of bounces actually
equals the logarithm base 2

00:05:56.200 --> 00:05:59.630
of 1 over the probability
of that symbol, which

00:05:59.630 --> 00:06:01.740
gives us our final equation.

00:06:01.740 --> 00:06:05.400
Entropy, or H, is the
summation for each symbol

00:06:05.400 --> 00:06:09.010
of the probability of that
symbol times the logarithm base

00:06:09.010 --> 00:06:12.750
2 of 1 over the
probability of that symbol.

00:06:12.750 --> 00:06:14.910
And Shannon writes this
slightly different,

00:06:14.910 --> 00:06:16.930
which just inverts
the expression

00:06:16.930 --> 00:06:20.500
inside the logarithm, which
causes us to add a negative,

00:06:20.500 --> 00:06:23.700
though both formulas
give the same results.

00:06:23.700 --> 00:06:25.470
So let's summarize.

00:06:25.470 --> 00:06:30.970
Entropy is maximum when all
outcomes are equally likely.

00:06:30.970 --> 00:06:34.470
Any time you move away from
equally likely outcomes

00:06:34.470 --> 00:06:38.670
or introduce predictability,
the entropy must go down.

00:06:38.670 --> 00:06:42.480
Now, the fundamental idea
is that if the entropy

00:06:42.480 --> 00:06:45.660
of an information
source drops, that

00:06:45.660 --> 00:06:49.770
means we can ask fewer
questions to guess the outcome.

00:06:49.770 --> 00:06:54.020
And thanks to Shannon, the bit--
which is the unit of entropy--

00:06:54.020 --> 00:06:56.820
is adopted as our
quantitative measure

00:06:56.820 --> 00:07:01.080
of information or
measure of surprise.

