[Script Info]
; Script generated by FFmpeg/Lavc56.60.100
ScriptType: v4.00+
PlayResX: 384
PlayResY: 288

[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding
Style: Default,Arial,16,&Hffffff,&Hffffff,&H0,&H0,0,0,0,0,100,100,0,0,1,1,0,2,10,10,10,0

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
Dialogue: 0,0:00:03.73,0:00:06.14,Default,,0,0,0,,Imagine 2 machines.
Dialogue: 0,0:00:06.14,0:00:12.96,Default,,0,0,0,,They both output messages from\Nan alphabet of A, B, C, or D.
Dialogue: 0,0:00:12.96,0:00:15.59,Default,,0,0,0,,Machine 1 generates\Neach symbol randomly.
Dialogue: 0,0:00:15.59,0:00:19.00,Default,,0,0,0,,They all occur 25% of the time.
Dialogue: 0,0:00:19.00,0:00:21.08,Default,,0,0,0,,Machine 2 generates\Nsymbols according
Dialogue: 0,0:00:21.08,0:00:22.37,Default,,0,0,0,,to the following probabilities.
Dialogue: 0,0:00:25.54,0:00:30.37,Default,,0,0,0,,Now, which machine is\Nproducing more information?
Dialogue: 0,0:00:30.37,0:00:34.43,Default,,0,0,0,,Claude Shannon cleverly\Nrephrased the question.
Dialogue: 0,0:00:34.43,0:00:39.20,Default,,0,0,0,,If you had to predict the\Nnext symbol from each machine,
Dialogue: 0,0:00:39.20,0:00:42.33,Default,,0,0,0,,what is the minimum number\Nof yes or no questions
Dialogue: 0,0:00:42.33,0:00:45.34,Default,,0,0,0,,you would expect to ask?
Dialogue: 0,0:00:45.34,0:00:47.20,Default,,0,0,0,,Let's look at Machine 1.
Dialogue: 0,0:00:47.20,0:00:50.02,Default,,0,0,0,,The most efficient way is\Nto pose a question which
Dialogue: 0,0:00:50.02,0:00:52.86,Default,,0,0,0,,divides the\Npossibilities in half.
Dialogue: 0,0:00:52.86,0:00:55.28,Default,,0,0,0,,For example, our\Nfirst question, we
Dialogue: 0,0:00:55.28,0:00:58.67,Default,,0,0,0,,could ask if it is any\N2 symbols-- such as,
Dialogue: 0,0:00:58.67,0:01:00.42,Default,,0,0,0,,is it A or B?
Dialogue: 0,0:01:00.42,0:01:03.35,Default,,0,0,0,,Since there is a\N50% chance of A or B
Dialogue: 0,0:01:03.35,0:01:08.02,Default,,0,0,0,,and a 50% chance of C or D.\NAfter getting the answer,
Dialogue: 0,0:01:08.02,0:01:10.76,Default,,0,0,0,,we can eliminate half\Nof the possibilities.
Dialogue: 0,0:01:10.76,0:01:15.26,Default,,0,0,0,,And we will be left with 2\Nsymbols, both equally likely.
Dialogue: 0,0:01:15.26,0:01:18.92,Default,,0,0,0,,So we simply pick\None-- such as, is it A?
Dialogue: 0,0:01:18.92,0:01:20.99,Default,,0,0,0,,And after this\Nsecond question, we
Dialogue: 0,0:01:20.99,0:01:23.57,Default,,0,0,0,,will have correctly\Nidentified the symbol.
Dialogue: 0,0:01:23.57,0:01:26.72,Default,,0,0,0,,So we can say the\Nuncertainty of Machine 1
Dialogue: 0,0:01:26.72,0:01:30.56,Default,,0,0,0,,is 2 questions per symbol.
Dialogue: 0,0:01:30.56,0:01:32.37,Default,,0,0,0,,Now, what about Machine 2?
Dialogue: 0,0:01:32.37,0:01:36.17,Default,,0,0,0,,As with Machine 1, we\Ncould ask two questions
Dialogue: 0,0:01:36.17,0:01:38.41,Default,,0,0,0,,to determine the next symbol.
Dialogue: 0,0:01:38.41,0:01:41.02,Default,,0,0,0,,However, this time the\Nprobability of each symbol
Dialogue: 0,0:01:41.02,0:01:41.98,Default,,0,0,0,,is different.
Dialogue: 0,0:01:41.98,0:01:45.06,Default,,0,0,0,,So we can ask our\Nquestions differently.
Dialogue: 0,0:01:45.06,0:01:48.50,Default,,0,0,0,,Here A has a 50%\Nchance of occurring,
Dialogue: 0,0:01:48.50,0:01:51.69,Default,,0,0,0,,and all other\Nletters add to 50%.
Dialogue: 0,0:01:51.69,0:01:55.07,Default,,0,0,0,,So we could start\Nby asking-- Is it A?
Dialogue: 0,0:01:55.07,0:01:57.33,Default,,0,0,0,,If it is A, we are done.
Dialogue: 0,0:01:57.33,0:01:59.86,Default,,0,0,0,,Only one question in this case.
Dialogue: 0,0:01:59.86,0:02:06.18,Default,,0,0,0,,Otherwise, we are left with 2\Nequal outcomes-- D or B and C.
Dialogue: 0,0:02:06.18,0:02:09.41,Default,,0,0,0,,So we could ask-- is it D?
Dialogue: 0,0:02:09.41,0:02:12.73,Default,,0,0,0,,If yes, we are done\Nwith 2 questions.
Dialogue: 0,0:02:12.73,0:02:15.44,Default,,0,0,0,,Otherwise, we have to\Nask a third question
Dialogue: 0,0:02:15.44,0:02:18.67,Default,,0,0,0,,to identify which of the\Nlast 2 symbols it is.
Dialogue: 0,0:02:23.39,0:02:26.45,Default,,0,0,0,,On average, how many\Nquestions do you
Dialogue: 0,0:02:26.45,0:02:31.69,Default,,0,0,0,,expect to ask to determine\Na symbol from Machine 2?
Dialogue: 0,0:02:31.69,0:02:34.94,Default,,0,0,0,,And this can be explained\Nnicely with an analogy.
Dialogue: 0,0:02:34.94,0:02:40.55,Default,,0,0,0,,Let's assume instead we want to\Nbuild Machine 1 and Machine 2.
Dialogue: 0,0:02:40.55,0:02:44.98,Default,,0,0,0,,And we can generate symbols\Nby bouncing a disk off a peg
Dialogue: 0,0:02:44.98,0:02:48.14,Default,,0,0,0,,into 1 of 2 equally\Nlikely directions.
Dialogue: 0,0:02:48.14,0:02:51.78,Default,,0,0,0,,Based on which way it falls,\Nwe can generate a symbol.
Dialogue: 0,0:02:51.78,0:02:54.06,Default,,0,0,0,,So with Machine\N1, we need to add
Dialogue: 0,0:02:54.06,0:02:56.75,Default,,0,0,0,,a second level or\Na second bounce
Dialogue: 0,0:02:56.75,0:03:00.00,Default,,0,0,0,,so that we have 2\Nbounces, which lead
Dialogue: 0,0:03:00.00,0:03:02.44,Default,,0,0,0,,to 4 equally likely outcomes.
Dialogue: 0,0:03:02.44,0:03:07.06,Default,,0,0,0,,And based on where the disk\Nlands, we output A, B, C, or D.
Dialogue: 0,0:03:07.06,0:03:08.65,Default,,0,0,0,,Now, Machine 2.
Dialogue: 0,0:03:08.65,0:03:10.54,Default,,0,0,0,,In this case, the\Nfirst bounce leads
Dialogue: 0,0:03:10.54,0:03:15.05,Default,,0,0,0,,to either an A-- which occurs\N50% of the time-- or else
Dialogue: 0,0:03:15.05,0:03:18.25,Default,,0,0,0,,we lead to a second bounce,\Nwhich then can either output
Dialogue: 0,0:03:18.25,0:03:22.02,Default,,0,0,0,,at D-- which occurs 25%\Nof the time-- or else
Dialogue: 0,0:03:22.02,0:03:25.74,Default,,0,0,0,,it leads to a third bounce,\Nwhich then leads to either B
Dialogue: 0,0:03:25.74,0:03:29.89,Default,,0,0,0,,or C-- 12.5% of the time.
Dialogue: 0,0:03:29.88,0:03:31.61,Default,,0,0,0,,So now we just take\Na weighted average
Dialogue: 0,0:03:31.61,0:03:35.13,Default,,0,0,0,,as follows-- the expected\Nnumber of bounces
Dialogue: 0,0:03:35.13,0:03:37.59,Default,,0,0,0,,is the probability\Nof symbol A times 1
Dialogue: 0,0:03:37.59,0:03:41.71,Default,,0,0,0,,bounce plus the probability\Nof B times 3 bounces
Dialogue: 0,0:03:41.71,0:03:44.54,Default,,0,0,0,,plus the probability\Nof C times 3 bounces
Dialogue: 0,0:03:44.54,0:03:47.88,Default,,0,0,0,,plus the probability\Nof D times 2 bounces.
Dialogue: 0,0:03:47.88,0:03:52.48,Default,,0,0,0,,And this works out\Nto 1.75 bounces.
Dialogue: 0,0:03:52.48,0:03:54.83,Default,,0,0,0,,Now, notice the\Nconnection between yes
Dialogue: 0,0:03:54.83,0:03:57.25,Default,,0,0,0,,or no questions\Nand fair bounces.
Dialogue: 0,0:03:57.25,0:04:00.12,Default,,0,0,0,,The expected number\Nof questions is
Dialogue: 0,0:04:00.12,0:04:03.92,Default,,0,0,0,,equal to the expected\Nnumber of bounces.
Dialogue: 0,0:04:03.92,0:04:07.78,Default,,0,0,0,,So Machine 1 requires 2\Nbounces to generate a symbol
Dialogue: 0,0:04:07.78,0:04:11.70,Default,,0,0,0,,while guessing an unknown\Nsymbol requires 2 questions.
Dialogue: 0,0:04:11.70,0:04:15.04,Default,,0,0,0,,Machine 2 requires 1.75 bounces.
Dialogue: 0,0:04:15.04,0:04:20.41,Default,,0,0,0,,We need to ask 1.75\Nquestions on average.
Dialogue: 0,0:04:20.41,0:04:24.36,Default,,0,0,0,,Meaning, if we need to guess\N100 symbols from both machines,
Dialogue: 0,0:04:24.36,0:04:28.92,Default,,0,0,0,,we can expect to ask 200\Nquestions for Machine 1
Dialogue: 0,0:04:28.92,0:04:33.16,Default,,0,0,0,,and 175 questions for Machine 2.
Dialogue: 0,0:04:33.16,0:04:38.05,Default,,0,0,0,,So this means that Machine 2\Nis producing less information
Dialogue: 0,0:04:38.05,0:04:40.79,Default,,0,0,0,,because there is\Nless uncertainty
Dialogue: 0,0:04:40.79,0:04:43.37,Default,,0,0,0,,or surprise about its output.
Dialogue: 0,0:04:43.37,0:04:44.57,Default,,0,0,0,,And that's it.
Dialogue: 0,0:04:44.57,0:04:48.86,Default,,0,0,0,,Claude Shannon calls this\Nmeasure of average uncertainty
Dialogue: 0,0:04:48.86,0:04:54.17,Default,,0,0,0,,"entropy," and he uses a\Nletter H to represent it.
Dialogue: 0,0:04:54.17,0:04:57.42,Default,,0,0,0,,And the unit of\Nentropy Shannon chooses
Dialogue: 0,0:04:57.42,0:05:01.03,Default,,0,0,0,,is based on the uncertainty\Nof a fair coin flip,
Dialogue: 0,0:05:01.03,0:05:03.75,Default,,0,0,0,,and he calls this\N"the bit," which
Dialogue: 0,0:05:03.75,0:05:05.35,Default,,0,0,0,,is equivalent to a fair bounce.
Dialogue: 0,0:05:08.64,0:05:11.20,Default,,0,0,0,,And we can arrive at the\Nsame result using our bounce
Dialogue: 0,0:05:11.20,0:05:12.23,Default,,0,0,0,,analogy.
Dialogue: 0,0:05:12.23,0:05:15.64,Default,,0,0,0,,Entropy, or H, is the\Nsummation for each symbol
Dialogue: 0,0:05:15.64,0:05:17.29,Default,,0,0,0,,of the probability\Nof that symbol
Dialogue: 0,0:05:17.29,0:05:19.67,Default,,0,0,0,,times the number of bounces.
Dialogue: 0,0:05:19.67,0:05:21.45,Default,,0,0,0,,Now, the difference\Nis-- how do we
Dialogue: 0,0:05:21.45,0:05:25.02,Default,,0,0,0,,express number of bounces\Nin a more general way?
Dialogue: 0,0:05:25.02,0:05:27.12,Default,,0,0,0,,And as we've seen,\Nnumber of bounces
Dialogue: 0,0:05:27.12,0:05:29.88,Default,,0,0,0,,depends how far down\Nthe tree we are.
Dialogue: 0,0:05:29.88,0:05:31.67,Default,,0,0,0,,And we can simplify\Nthis by saying
Dialogue: 0,0:05:31.67,0:05:37.01,Default,,0,0,0,,that the number of bounces\Nequals the logarithm base 2
Dialogue: 0,0:05:37.01,0:05:39.18,Default,,0,0,0,,of the number of\Noutcomes at that level.
Dialogue: 0,0:05:39.18,0:05:41.77,Default,,0,0,0,,And the number of\Noutcomes at a level
Dialogue: 0,0:05:41.77,0:05:44.82,Default,,0,0,0,,is also based on the\Nprobability, where
Dialogue: 0,0:05:44.82,0:05:46.83,Default,,0,0,0,,the number of\Noutcomes at a level
Dialogue: 0,0:05:46.83,0:05:50.81,Default,,0,0,0,,equals 1 divided by the\Nprobability of that outcome.
Dialogue: 0,0:05:50.81,0:05:56.20,Default,,0,0,0,,Number of bounces actually\Nequals the logarithm base 2
Dialogue: 0,0:05:56.20,0:05:59.63,Default,,0,0,0,,of 1 over the probability\Nof that symbol, which
Dialogue: 0,0:05:59.63,0:06:01.74,Default,,0,0,0,,gives us our final equation.
Dialogue: 0,0:06:01.74,0:06:05.40,Default,,0,0,0,,Entropy, or H, is the\Nsummation for each symbol
Dialogue: 0,0:06:05.40,0:06:09.01,Default,,0,0,0,,of the probability of that\Nsymbol times the logarithm base
Dialogue: 0,0:06:09.01,0:06:12.75,Default,,0,0,0,,2 of 1 over the\Nprobability of that symbol.
Dialogue: 0,0:06:12.75,0:06:14.91,Default,,0,0,0,,And Shannon writes this\Nslightly different,
Dialogue: 0,0:06:14.91,0:06:16.93,Default,,0,0,0,,which just inverts\Nthe expression
Dialogue: 0,0:06:16.93,0:06:20.50,Default,,0,0,0,,inside the logarithm, which\Ncauses us to add a negative,
Dialogue: 0,0:06:20.50,0:06:23.70,Default,,0,0,0,,though both formulas\Ngive the same results.
Dialogue: 0,0:06:23.70,0:06:25.47,Default,,0,0,0,,So let's summarize.
Dialogue: 0,0:06:25.47,0:06:30.97,Default,,0,0,0,,Entropy is maximum when all\Noutcomes are equally likely.
Dialogue: 0,0:06:30.97,0:06:34.47,Default,,0,0,0,,Any time you move away from\Nequally likely outcomes
Dialogue: 0,0:06:34.47,0:06:38.67,Default,,0,0,0,,or introduce predictability,\Nthe entropy must go down.
Dialogue: 0,0:06:38.67,0:06:42.48,Default,,0,0,0,,Now, the fundamental idea\Nis that if the entropy
Dialogue: 0,0:06:42.48,0:06:45.66,Default,,0,0,0,,of an information\Nsource drops, that
Dialogue: 0,0:06:45.66,0:06:49.77,Default,,0,0,0,,means we can ask fewer\Nquestions to guess the outcome.
Dialogue: 0,0:06:49.77,0:06:54.02,Default,,0,0,0,,And thanks to Shannon, the bit--\Nwhich is the unit of entropy--
Dialogue: 0,0:06:54.02,0:06:56.82,Default,,0,0,0,,is adopted as our\Nquantitative measure
Dialogue: 0,0:06:56.82,0:07:01.08,Default,,0,0,0,,of information or\Nmeasure of surprise.
